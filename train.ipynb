{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import tensorflow.contrib.keras as kr\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from config import Config\n",
    "from func import *\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _highway_layer(input_, size, num_layers=1):\n",
    "    \"\"\"\n",
    "    Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "    for idx in range(num_layers):\n",
    "        fc0 = tf.layers.dense(input_, size, activation=tf.nn.relu)\n",
    "        fc1 = tf.layers.dense(fc0, size, activation=tf.sigmoid)\n",
    "        output = fc0 * fc1 + (1. - fc1) * input_\n",
    "        input_ = output\n",
    "    return output\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"文本分类，CNN模型\"\"\"\n",
    "    def __init__(self, saving_loading = False):\n",
    "        # 三个待输入的数据\n",
    "        tf.reset_default_graph()\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, Config.seq_length], name='input_x')\n",
    "        # self.input_x = tf.placeholder(tf.float32, [None, self.Config.seq_length], name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, Config.num_classes], name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        self.cnn()\n",
    "        self.session = tf.Session()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        if saving_loading == True:\n",
    "            self.save_dir = 'checkpoints/textcnn'\n",
    "            self.save_path = os.path.join(self.save_dir, 'best_validation')\n",
    "            self.saver = load_model(self.session, self.save_dir)\n",
    "        self.saving_or_loading = saving_loading\n",
    "    def cnn(self):\n",
    "        \"\"\"CNN模型\"\"\"\n",
    "        # 词向量映射\n",
    "        with tf.name_scope('embedding'):\n",
    "            embedding = tf.get_variable('embedding', [Config.vocab_size, Config.embedding_dim])\n",
    "            embedding_outputs = tf.nn.embedding_lookup(embedding, self.input_x)        \n",
    "#         with tf.name_scope(\"Bi_lsm\"):\n",
    "#             lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(Config.lstm_hidden_size)\n",
    "#             lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(Config.lstm_hidden_size)\n",
    "#             lstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=Config.dropout_keep_prob)\n",
    "#             lstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=Config.dropout_keep_prob)\n",
    "#             outputs_lstm, state_lstm = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, lstm_bw_cell, embedding_outputs, dtype=tf.float32)\n",
    "#             lstm_concat_front = tf.concat(outputs_lstm, axis=2)\n",
    "        pooled_outputs = []\n",
    "        for filter_size in Config.kernel_size:\n",
    "            with tf.name_scope('conv_filter{0}'.format(filter_size)):                \n",
    "                conv1 = tf.layers.conv1d(embedding_outputs, 128,kernel_size=filter_size,strides=1, padding='SAME')                \n",
    "#                 conv_bn_output = tf.layers.batch_normalization(conv1)\n",
    "                conv_re_output = tf.nn.relu(conv1)\n",
    "            with tf.name_scope(\"pool_filter{0}\".format(filter_size)):\n",
    "                # pooled = tf.layers.max_pooling2d(conv_re_output,pool_size=[self.Config.seq_length-filter_size+1, 1],strides=1, padding='VALID', name=\"pool\")\n",
    "                pooled0 = tf.reduce_max(conv_re_output, reduction_indices=[1])\n",
    "                pooled1 = tf.reduce_mean(conv_re_output, reduction_indices=[1])\n",
    "                pooled_combine = tf.concat([pooled0, pooled1], axis=-1)\n",
    "            pooled_outputs.append(pooled_combine)\n",
    "        h_pool = tf.concat(pooled_outputs, axis=1)\n",
    "        # h_pool_fl = tf.layers.flatten(h_pool)\n",
    "#         with tf.name_scope('fc'):\n",
    "#             fc0 = tf.layers.dense(h_pool, 512, name='fc0',kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "#             fc0 = tf.contrib.layers.dropout(fc0, self.keep_prob)\n",
    "#             fc0_out = tf.nn.relu(fc0)\n",
    "#         with tf.name_scope(\"highway\"):\n",
    "#             highway = _highway_layer(h_pool, h_pool.get_shape()[1], num_layers=2)\n",
    "            # fc_input1 = tf.layers.dense(fc_input0, 256, name='fc1', kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            #                             activation=tf.nn.relu)\n",
    "            # fc_input0 = tf.layers.batch_normalization(fc_input0)\n",
    "        #     gmp = tf.reduce_max(conv, reduction_indices=[1], name='gmp')\n",
    "\n",
    "        with tf.name_scope(\"score\"):\n",
    "            # 全连接层，后面接dropout以及relu激活\n",
    "#             fc = tf.layers.dense(h_pool, 64, name='fc2')\n",
    "#             fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n",
    "#             fc = tf.nn.relu(fc)\n",
    "\n",
    "            # 分类器\n",
    "            self.logits = tf.layers.dense(h_pool, Config.num_classes, name='fc3')\n",
    "            self.soft = tf.nn.softmax(self.logits)\n",
    "            self.y_pred_cls = tf.argmax(self.soft, 1)  # 预测类别\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # 损失函数，交叉熵\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # 优化器\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=Config.learning_rate).minimize(self.loss)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # 准确率\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            self.label = tf.argmax(self.input_y, 1)\n",
    "# a = TCNNConfig()\n",
    "# b = TextCNN(a)\n",
    "\n",
    "def get_time_dif(start_time):\n",
    "    \"\"\"获取已使用时间\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_dif)))\n",
    "\n",
    "\n",
    "def process(X, Y, max_length=700, class_count=19):\n",
    "    \"\"\"将dataframeX,Y的训练数据提取出来\"\"\"\n",
    "    # X = X['word_seg'].values\n",
    "    # Y = Y.values\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = kr.preprocessing.sequence.pad_sequences(X, max_length,padding='post',truncating='post')\n",
    "    y_pad = kr.utils.to_categorical(Y, num_classes=class_count)  # 将标签转换为one-hot表示\n",
    "    return x_pad, y_pad\n",
    "# def process(X, Y, max_length=600, class_count=19):\n",
    "#     \"\"\"将dataframeX,Y的训练数据提取出来\"\"\"\n",
    "#     # X = X['word_seg'].values\n",
    "#     Y = Y\n",
    "#     # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "#     # x_pad = kr.preprocessing.sequence.pad_sequences(X, max_length,padding='post',truncating='post')\n",
    "#     y_pad = kr.utils.to_categorical(Y, num_classes=class_count)  # 将标签转换为one-hot表示\n",
    "#     return X, y_pad\n",
    "\n",
    "def load_model(sess, path):\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint = tf.train.get_checkpoint_state(path)\n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old weights!\")\n",
    "    return saver\n",
    "\n",
    "def train_network(train_x, train_y, val_x, val_y, saving_loading = False):\n",
    "    model = TextCNN()\n",
    "    # print(\"Configuring TensorBoard and Saver...\")\n",
    "    # 配置 Tensorboard，重新训练时，请将tensorboard文件夹删除，不然图会覆盖\n",
    "    # tensorboard_dir = 'tensorboard/textcnn'\n",
    "    # if not os.path.exists(tensorboard_dir):\n",
    "    #     os.makedirs(tensorboard_dir)\n",
    "    #\n",
    "    # tf.summary.scalar(\"loss\", model.loss)\n",
    "    # tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    # merged_summary = tf.summary.merge_all()\n",
    "    # writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "    res_model = model\n",
    "    # 配置 Saver 或者导入weight\n",
    "    print(\"Loading training and validation data...\")\n",
    "    # 载入训练集与验证集\n",
    "    start_time = time.time()\n",
    "    x_train, y_train = process(train_x, train_y, Config.seq_length)\n",
    "    x_val, y_val = process(val_x, val_y, Config.seq_length)\n",
    "    # x_train, y_train = train_x, train_y\n",
    "    # x_val, y_val = val_x, val_y\n",
    "    time_dif = get_time_dif(start_time)\n",
    "    print(\"Time usage:\", time_dif)\n",
    "\n",
    "    # 创建session\n",
    "    # writer.add_graph(session.graph)\n",
    "    print('Training and evaluating...')\n",
    "    start_time = time.time()\n",
    "    total_batch = 0  # 总批次\n",
    "    best_F1 = 0.0  # 最佳验证集准确率\n",
    "    last_improved = 0  # 记录上一次提升批次\n",
    "    require_improvement = 300000  # 如果超过1000轮未提升，提前结束训练\n",
    "    flag = False\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        print('Epoch:', epoch + 1)\n",
    "        # tf.reset_default_graph()  # 重置默认图\n",
    "        # graph = tf.Graph()  # 新建空白图\n",
    "        # with graph.as_default() as g:  # 将新建的图作为默认图\n",
    "        #     with tf.Session(graph=g) as session:  # Session  在新建的图中运行\n",
    "\n",
    "        # 需要运行的代码放这里，每次运行都会使用新的图\n",
    "        batch_train = batch_iter(x_train, y_train, Config.batch_size)\n",
    "        for x_batch, y_batch in batch_train:\n",
    "            feed_dict = feed_data(x_batch, y_batch, Config.dropout_keep_prob, model)\n",
    "            # if total_batch % Config.save_per_batch == 0:\n",
    "                # 每多少轮次将训练结果写入tensorboard scalar\n",
    "                # s = session.run(merged_summary, feed_dict=feed_dict)\n",
    "                # writer.add_summary(s, total_batch)\n",
    "            if total_batch % Config.save_per_batch == 0:\n",
    "                mes = \"Iter: {0:>6}, Time: {1}\"\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                sys.stdout.flush()\n",
    "                print(mes.format(total_batch, time_dif))\n",
    "            if total_batch % Config.print_per_batch == 0:\n",
    "                # 每多少轮次输出在训练集和验证集上的性能\n",
    "                feed_dict[model.keep_prob] = 1.0\n",
    "                loss_train, acc_train = model.session.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "                loss_val, F1, ACC = evaluate(x_val, y_val, model)  # todo\n",
    "                for i in range(1):\n",
    "                    print(\" * \")\n",
    "                    sys.stdout.flush()\n",
    "                    time.sleep(1)\n",
    "                if F1 > best_F1:\n",
    "                    # 保存最好结果\n",
    "                    best_F1 = F1\n",
    "                    last_improved = total_batch\n",
    "                    if model.saving_or_loading == True:\n",
    "                        model.saver.save(sess=model.session, save_path=model.save_path)\n",
    "                    res_model = model\n",
    "                    improved_str = '*'\n",
    "                    sys.stdout.flush()\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "\n",
    "                time_dif = get_time_dif(start_time)\n",
    "                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val F1: {4:>7.2%},Val Acc: {5:>7.2%} Time: {6} {7}'\n",
    "                print(msg.format(total_batch, loss_train, acc_train, loss_val, F1, ACC, time_dif, improved_str))\n",
    "\n",
    "            model.session.run(model.optim, feed_dict=feed_dict)  # 运行优化\n",
    "            total_batch += 1\n",
    "\n",
    "            if total_batch - last_improved > require_improvement:\n",
    "                # 验证集正确率长期不提升，提前结束训练\n",
    "                print(\"No optimization for a long time, auto-stopping...\")\n",
    "                flag = True\n",
    "                break  # 跳出循环\n",
    "        if flag:  # 同上\n",
    "            break\n",
    "    return res_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path0 = 'Data/result_train_embedding_word.pkl'\n",
    "path1 = 'Data/result_test_embedding_word.pkl'\n",
    "# path0 = 'Data/result_train_embedding_article.pkl'\n",
    "# path1 = 'Data/result_test_embedding_article.pkl'\n",
    "train = pd.read_csv('../new_data/data/train_set.csv')\n",
    "test = pd.read_csv('../new_data/data/test_set.csv')\n",
    "column = \"word_seg\"\n",
    "train['word_seg'] = train['word_seg'].map(lambda t: np.array(t.split(' ')).astype(int))\n",
    "train_x_I = train['word_seg'].values\n",
    "test['word_seg'] = test['word_seg'].map(lambda t: np.array(t.split(' ')).astype(int))\n",
    "test_x_I = test['word_seg'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = ((train[\"class\"])-1).astype(int)\n",
    "kf = KFold(n_splits=5)\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training and validation data...\n",
      "Time usage: 0:00:01\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "Iter:      0, Time: 0:00:00\n",
      " * \n",
      "Iter:      0, Train Loss:    2.9, Train Acc:  12.50%, Val Loss:    2.9, Val F1:   0.86%,Val Acc:   0.57% Time: 0:00:02 *\n",
      "Iter:     50, Time: 0:00:04\n",
      "Iter:    100, Time: 0:00:06\n",
      "Iter:    150, Time: 0:00:08\n",
      "Iter:    200, Time: 0:00:11\n",
      "Iter:    250, Time: 0:00:13\n",
      "Iter:    300, Time: 0:00:15\n",
      "Iter:    350, Time: 0:00:17\n",
      "Iter:    400, Time: 0:00:19\n",
      "Iter:    450, Time: 0:00:22\n",
      "Iter:    500, Time: 0:00:24\n",
      " * \n",
      "Iter:    500, Train Loss:    1.2, Train Acc:  62.50%, Val Loss:    1.0, Val F1:  68.25%,Val Acc:  70.07% Time: 0:00:25 *\n",
      "Iter:    550, Time: 0:00:28\n",
      "Iter:    600, Time: 0:00:30\n",
      "Iter:    650, Time: 0:00:32\n",
      "Iter:    700, Time: 0:00:34\n",
      "Iter:    750, Time: 0:00:37\n",
      "Iter:    800, Time: 0:00:39\n",
      "Iter:    850, Time: 0:00:41\n",
      "Iter:    900, Time: 0:00:43\n",
      "Iter:    950, Time: 0:00:45\n",
      "Iter:   1000, Time: 0:00:48\n",
      " * \n",
      "Iter:   1000, Train Loss:   0.74, Train Acc:  78.12%, Val Loss:   0.93, Val F1:  71.93%,Val Acc:  72.93% Time: 0:00:49 *\n",
      "Iter:   1050, Time: 0:00:51\n",
      "Iter:   1100, Time: 0:00:54\n",
      "Iter:   1150, Time: 0:00:56\n",
      "Iter:   1200, Time: 0:00:58\n",
      "Epoch: 2\n",
      "Iter:   1250, Time: 0:01:00\n",
      "Iter:   1300, Time: 0:01:03\n",
      "Iter:   1350, Time: 0:01:05\n",
      "Iter:   1400, Time: 0:01:07\n",
      "Iter:   1450, Time: 0:01:09\n",
      "Iter:   1500, Time: 0:01:11\n",
      " * \n",
      "Iter:   1500, Train Loss:   0.41, Train Acc:  87.50%, Val Loss:   0.95, Val F1:  72.10%,Val Acc:  73.58% Time: 0:01:13 *\n",
      "Iter:   1550, Time: 0:01:15\n",
      "Iter:   1600, Time: 0:01:17\n",
      "Iter:   1650, Time: 0:01:20\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "Config.kernel_size = [2,3,4,5]\n",
    "Config.lstm_hidden_size = 128\n",
    "Config.seq_length = 1000\n",
    "for train_inx, val_inx in kf.split(train_x_I):\n",
    "    train_x_b = train_x_I[train_inx]\n",
    "    y_b = y[train_inx]\n",
    "    train_x = train_x_b[:int((train_x_b.shape[0])*0.95)]\n",
    "    train_y = y_b[:int((train_x_b.shape[0])*0.95)]\n",
    "    val_x = train_x_b[int((train_x_b.shape[0])*0.95):]\n",
    "    val_y = y_b[int((train_x_b.shape[0])*0.95):]\n",
    "    # train_x = train_x_b[:int((train_x_b.shape[0])*0.01)]\n",
    "    # train_y = y_b[:int((train_x_b.shape[0])*0.01)]\n",
    "    # val_x = train_x_b[int((train_x_b.shape[0])*0.99):]\n",
    "    # val_y = y_b[int((train_x_b.shape[0])*0.99):]\n",
    "    MODEL = train_network(train_x, train_y, val_x, val_y)\n",
    "    generate_x = train_x_I[val_inx]\n",
    "    generate_x = kr.preprocessing.sequence.pad_sequences(generate_x, Config.seq_length, padding='post', truncating='post')\n",
    "    result.append(generate_result(generate_x, MODEL))\n",
    "    del MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
